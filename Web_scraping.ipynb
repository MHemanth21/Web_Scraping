{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib3\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Disable SSL certificate warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = \"https://www.rehabmart.com\"\n",
    "CATEGORY_URL = \"https://www.rehabmart.com/category/abdominal_supports.htm\"\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:53.0) Gecko/20100101 Firefox/53.0\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0\"\n",
    "]\n",
    "\n",
    "# ScraperAPI Key for proxy\n",
    "SCRAPERAPI_KEY = \"a7939ea9627f8a0322ded8ad05bbe245\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_user_agents():\n",
    "    \"\"\"Randomly select a User-Agent from the list.\"\"\"\n",
    "    return random.choice(USER_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proxy():\n",
    "    proxies = {\n",
    "        \"http\": f\"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001\",\n",
    "        \"https\": f\"http://scraperapi:{SCRAPERAPI_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "    }\n",
    "    return proxies\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=2, status_forcelist=[500, 502, 503, 504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url):\n",
    "    \"\"\"Fetch a URL and return the response text.\"\"\"\n",
    "    headers = {\"User-Agent\": rotate_user_agents()}\n",
    "    try:\n",
    "        response = session.get(\n",
    "            url, \n",
    "            headers=headers, \n",
    "            proxies=get_proxy(), \n",
    "            timeout=15, \n",
    "            verify=False\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_urls(category_url):\n",
    "    \"\"\"Fetch all product URLs from a given category, including pagination.\"\"\"\n",
    "    print(f\"Fetching products from: {category_url}\")\n",
    "    product_urls = []\n",
    "    \n",
    "    while category_url:\n",
    "        html = fetch_url(category_url)\n",
    "        if not html:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Check if the product grid exists (using id=\"categorygrid\")\n",
    "        product_grid = soup.find(\"div\", id=\"categorygrid\")\n",
    "        if product_grid:\n",
    "            product_boxes = product_grid.find_all(\"div\", class_=\"col-xs-6 col-sm-3 col-md-3 boxed productTD\")\n",
    "            for box in product_boxes:\n",
    "                product_link = box.find(\"div\", class_=\"findproducttitle producttitle\").find(\"a\")[\"href\"]\n",
    "                # Ensure the URL is correctly formed\n",
    "                if product_link.startswith(\"/\"):\n",
    "                    product_urls.append(f\"{BASE_URL}{product_link}\")\n",
    "                else:\n",
    "                    product_urls.append(product_link)\n",
    "\n",
    "        # Check for the second pagination div\n",
    "        pagination = soup.find_all(\"div\", class_=\"pagination\")\n",
    "        if len(pagination) > 1:  # Ensure there are multiple pagination divs\n",
    "            next_page = pagination[1].find(\"a\", string=\"Next\")\n",
    "            if next_page:\n",
    "                category_url = f\"{BASE_URL}{next_page['href']}\"  # Navigate to next page\n",
    "            else:\n",
    "                break  # No next page, stop the loop\n",
    "        else:\n",
    "            break  # No pagination div found, stop the loop\n",
    "\n",
    "    return product_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_info(url):\n",
    "    \"\"\"Extract product details from a product URL.\"\"\"\n",
    "    html = fetch_url(url)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    try:\n",
    "        name = soup.find(\"h1\", itemprop=True).text.strip()\n",
    "    except AttributeError:\n",
    "        name = None\n",
    "\n",
    "    try:\n",
    "        breadcrumb_list = soup.find(\"ol\", class_=\"breadcrumb hidden-xsx\").find_all(\"li\")\n",
    "        category = breadcrumb_list[1].find(\"span\", itemprop=\"name\").text.strip() if len(breadcrumb_list) > 1 else None\n",
    "    except AttributeError:\n",
    "        category = None\n",
    "\n",
    "    try:\n",
    "        list_price = soup.find(\"s\", style=\"font-size:1.2em;\").text.strip()\n",
    "    except AttributeError:\n",
    "        list_price = None\n",
    "\n",
    "    try:\n",
    "        special_price = soup.find(\"div\", class_=\"text-danger\").find(\"b\").find(\"span\", class_=\"text-danger price\").text.strip()\n",
    "    except AttributeError:\n",
    "        special_price = None\n",
    "\n",
    "    try:\n",
    "        description = soup.find(\"div\", itemprop=\"description\").text.strip()\n",
    "        description = re.sub(r\"^(Product Overview:\\s*)\", \"\", description, flags=re.I).strip()\n",
    "    except AttributeError:\n",
    "        description = None\n",
    "\n",
    "    try:\n",
    "        rating_text = soup.find(\"div\", class_=\"starreview-container\").find(\"strong\").text.strip()\n",
    "        rating_match = re.search(r\"(\\d(?:\\.\\d)?) of (\\d) stars\", rating_text)\n",
    "        rating = f\"{rating_match.group(1)} out of {rating_match.group(2)} stars\" if rating_match else None\n",
    "    except AttributeError:\n",
    "        rating = None\n",
    "\n",
    "    try:\n",
    "        img_url = soup.find(\"div\", class_=\"gallery\").find(\"img\", class_=\"img-responsive\")[\"src\"]\n",
    "    except AttributeError:\n",
    "        img_url = None\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"category\": category,\n",
    "        \"list_price\": list_price,\n",
    "        \"special_price\": special_price,\n",
    "        \"description\": description,\n",
    "        \"rating\": rating,\n",
    "        \"img_url\": img_url,\n",
    "        \"product_url\": url\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_products_from_category(category_url):\n",
    "    \"\"\"Scrape all products from a single category and return all product data.\"\"\"\n",
    "    product_urls = get_product_urls(category_url)\n",
    "    all_product_data = []\n",
    "\n",
    "    for url in tqdm(product_urls, desc=\"Fetching Product Details\"):\n",
    "        product_info = extract_product_info(url)\n",
    "        if product_info:\n",
    "            all_product_data.append(product_info)\n",
    "\n",
    "    return all_product_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_urls():\n",
    "    \"\"\"Get all category URLs from the RehabMart categories page.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(\"https://www.rehabmart.com/all-categories.asp\")\n",
    "\n",
    "    # Wait for elements to load\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    category_urls = set()\n",
    "\n",
    "    # Locate all category list items\n",
    "    category_elements = driver.find_elements(By.CSS_SELECTOR, \"div.panel-body.cats ul.ul-mobile.cat-list li a\")\n",
    "    \n",
    "    for category in category_elements:\n",
    "        url = category.get_attribute(\"href\")\n",
    "        if url:\n",
    "            category_urls.add(url)\n",
    "\n",
    "    driver.quit()\n",
    "    return list(category_urls)\n",
    "\n",
    "# Get all category URLs\n",
    "category_urls = get_category_urls()\n",
    "\n",
    "# Scrape product details from each category and collect all the product data\n",
    "all_product_data = []\n",
    "for category_url in category_urls:\n",
    "    category_data = scrape_products_from_category(category_url)\n",
    "    all_product_data.extend(category_data)\n",
    "\n",
    "# Save all product details to a single CSV file\n",
    "df = pd.DataFrame(all_product_data)\n",
    "df.to_csv(\"rehabmart.csv\", index=False)\n",
    "print(\"Data saved to rehabmart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
